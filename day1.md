100 Days Challenge - Day 1.
Linear Regression, Logistic Regression and Neural Networks.

Revised Week 1 to Week 4 of Andrew Ng's Machine Learning course on Coursera which I had already completed a few months ago.


Topics covered include:


Introduction to Machine Learning

    Supervised
    Unsupervised

Linear Regression in One Variable

    Model Representation
    Cost Function
    Gradient Descent

 Linear Regression in Multiple Variables

    Model Representation
    Gradient Descent
    Feature Scaling
    Learning Rate
    Polynomial Regression
    Normal Equation
    Normal Equation vs Gradient Descent
    Normal Equation Non-Invertibility

 Logistic Regression

    Hypothesis Representation
    Sigmoid Function
    Decision Boundary (Linear, Non-Linear)
    Cost Function and Gradient Descent
    Other Optimization Algorithms (Conjugate Gradient, BFGS, L-BFGS)
    One vs All Multi-class Classification

 Regularization

    Overfitting
    Correction in Cost Function (Regularization)
    Gradient Descent
    Normal Equation and Invertibility

Neural Networks

    Examples and Intuitions
    Multi-class Classification


Goal for Tomorrow: Revise Backpropagation and implement a Multi-Class Classification from scratch
