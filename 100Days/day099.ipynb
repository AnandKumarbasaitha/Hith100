{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"ltr\" style=\"text-align: left;\" trbidi=\"on\">\n",
    "<br />\n",
    "<h2 style=\"text-align: left;\">\n",
    "100 Days of ML Day 99</h2>\n",
    "<h2 style=\"text-align: left;\">\n",
    "Chat vs Article Text Classifier</h2>\n",
    "Built a text classifier (into chat and article) very similar to the one discussed in the article 'Naive Bayes Classifier for Text Classification' by Jaya Aiyappan (classifying sentences into questions and statements.)<br />\n",
    "<br />\n",
    "Code: <a href=\"https://github.com/hithesh111/Hith100/blob/master/100Days/day099.ipynb\">https://github.com/hithesh111/Hith100/blob/master/100Days/day099.ipynb</a><br />\n",
    "<br />\n",
    "Article:<a href=\"https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b\">https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b</a></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prod_list(x):\n",
    "    log_prod = 0\n",
    "    for i in range(len(x)):\n",
    "        log_prod = log_prod - math.log(x[i])\n",
    "    return log_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I mean we spammed earlier for over an hour and...</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yoyoyo i love singing these songs</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>okay i failed to sound cool again whats good</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 blondes walk into a bar..you think they woul...</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What time is it in your country?</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rising temperatures are melting permafrost, re...</td>\n",
       "      <td>Article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This absence of (the passing of) time is a cen...</td>\n",
       "      <td>Article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No matter what measure you put in place, edge ...</td>\n",
       "      <td>Article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In writing this piece, I spoke with Black moth...</td>\n",
       "      <td>Article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Of course, you might want to purchase a DIY mo...</td>\n",
       "      <td>Article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Let's all just spam until people get off the c...</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>guess no hot chicks here</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I'M JUST A COOL BOY NOBODY LOVES ME. IS</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>right now im making fan art of study girl and ...</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Most likely there won’t be anyone colecting me...</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Happy Birthday Neeraj!</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>We want to begin the decoration for CHAOS as s...</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>If he went out for cigarettes, he's not coming...</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>KPOP IS NOT A RELIGION NOW EVERYONE SHUT UP</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>THIS MUSIC IS TO RELAX. MOST OF U ARE MAD LMAO</td>\n",
       "      <td>Online Chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence        Class\n",
       "0   I mean we spammed earlier for over an hour and...  Online Chat\n",
       "1                   yoyoyo i love singing these songs  Online Chat\n",
       "2        okay i failed to sound cool again whats good  Online Chat\n",
       "3   2 blondes walk into a bar..you think they woul...  Online Chat\n",
       "4                    What time is it in your country?  Online Chat\n",
       "5   Rising temperatures are melting permafrost, re...      Article\n",
       "6   This absence of (the passing of) time is a cen...      Article\n",
       "7   No matter what measure you put in place, edge ...      Article\n",
       "8   In writing this piece, I spoke with Black moth...      Article\n",
       "9   Of course, you might want to purchase a DIY mo...      Article\n",
       "10  Let's all just spam until people get off the c...  Online Chat\n",
       "11                           guess no hot chicks here  Online Chat\n",
       "12            I'M JUST A COOL BOY NOBODY LOVES ME. IS  Online Chat\n",
       "13  right now im making fan art of study girl and ...  Online Chat\n",
       "14  Most likely there won’t be anyone colecting me...  Online Chat\n",
       "15                             Happy Birthday Neeraj!  Online Chat\n",
       "16  We want to begin the decoration for CHAOS as s...  Online Chat\n",
       "17  If he went out for cigarettes, he's not coming...  Online Chat\n",
       "18        KPOP IS NOT A RELIGION NOW EVERYONE SHUT UP  Online Chat\n",
       "19     THIS MUSIC IS TO RELAX. MOST OF U ARE MAD LMAO  Online Chat"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/home/hithesh/Documents/CHATvsARTICLE.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    again  all  an  and  any  anyone  are  art  as  back  ...  what  whats  \\\n",
       " 0       0    1   1    1    0       0    0    0   0     0  ...     0      0   \n",
       " 1       0    0   0    0    0       0    0    0   0     0  ...     0      0   \n",
       " 2       1    0   0    0    0       0    0    0   0     0  ...     0      1   \n",
       " 3       0    0   0    0    0       0    0    0   0     0  ...     0      0   \n",
       " 4       0    0   0    0    0       0    0    0   0     0  ...     1      0   \n",
       " 5       2    2   0    0    0       0    0    0   0     0  ...     0      0   \n",
       " 6       0    0   0    0    0       0    0    0   0     0  ...     0      0   \n",
       " 7       0    0   0    0    0       0    0    0   0     0  ...     0      0   \n",
       " 8       0    0   0    1    0       0    0    1   0     0  ...     0      0   \n",
       " 9       0    0   0    0    1       1    0    0   0     0  ...     0      0   \n",
       " 10      0    0   0    0    0       0    0    0   0     0  ...     0      0   \n",
       " 11      0    0   0    0    0       0    0    0   2     0  ...     0      0   \n",
       " 12      0    0   0    0    0       0    0    0   0     1  ...     0      0   \n",
       " 13      0    0   0    0    0       0    0    0   0     0  ...     0      0   \n",
       " 14      0    0   0    0    0       0    1    0   0     0  ...     0      0   \n",
       " \n",
       "     will  won  world  would  you  your  yourself  yoyoyo  \n",
       " 0      0    0      0      0    0     0         0       0  \n",
       " 1      0    0      0      0    0     0         0       1  \n",
       " 2      0    0      0      0    0     0         0       0  \n",
       " 3      0    0      0      1    1     0         0       0  \n",
       " 4      0    0      0      0    0     1         0       0  \n",
       " 5      0    0      1      0    0     0         0       0  \n",
       " 6      0    0      0      0    0     0         0       0  \n",
       " 7      0    0      0      0    0     0         0       0  \n",
       " 8      0    0      0      0    0     0         0       0  \n",
       " 9      1    1      0      0    0     0         1       0  \n",
       " 10     0    0      0      0    0     0         0       0  \n",
       " 11     0    0      0      0    0     0         0       0  \n",
       " 12     0    0      0      0    0     0         0       0  \n",
       " 13     0    0      0      0    0     0         0       0  \n",
       " 14     0    0      0      0    0     0         0       0  \n",
       " \n",
       " [15 rows x 142 columns],\n",
       "    absence  across  actually  age  akiba  akihabara  almost  also  always  an  \\\n",
       " 0        0       0         0    0      0          0       0     1       0   0   \n",
       " 1        1       0         0    1      0          0       0     1       0   0   \n",
       " 2        0       0         0    0      0          0       1     0       1   0   \n",
       " 3        0       0         0    0      0          0       0     0       0   0   \n",
       " 4        0       1         1    0      1          1       0     0       0   1   \n",
       " \n",
       "    ...  while  who  will  willing  with  writing  yodobashi  you  your  \\\n",
       " 0  ...      0    0     0        0     1        0          0    0     0   \n",
       " 1  ...      0    0     0        0     0        0          0    0     0   \n",
       " 2  ...      1    0     1        0     0        0          0    1     0   \n",
       " 3  ...      0    1     0        1     2        1          0    1     2   \n",
       " 4  ...      0    0     0        0     0        0          1    3     0   \n",
       " \n",
       "    yourself  \n",
       " 0         0  \n",
       " 1         0  \n",
       " 2         0  \n",
       " 3         0  \n",
       " 4         1  \n",
       " \n",
       " [5 rows x 169 columns])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_docs = [row['Sentence'] for index,row in data.iterrows() if row['Class'] == 'Online Chat']\n",
    "article_docs = [row['Sentence'] for index,row in data.iterrows() if row['Class'] == 'Article']\n",
    "\n",
    "c_vec = CountVectorizer()\n",
    "\n",
    "X_chat = c_vec.fit_transform(chat_docs)\n",
    "tdm_chat = pd.DataFrame(X_chat.toarray(), columns=c_vec.get_feature_names())\n",
    "\n",
    "X_article = c_vec.fit_transform(article_docs)\n",
    "tdm_article = pd.DataFrame(X_article.toarray(), columns=c_vec.get_feature_names())\n",
    "\n",
    "tdm_chat,tdm_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'absence': 3, 'across': 3, 'actually': 1, 'age': 2, 'akiba': 1, 'akihabara': 1, 'almost': 1, 'also': 1, 'always': 2, 'an': 1, 'and': 1, 'are': 1, 'as': 2, 'ask': 1, 'asked': 1, 'assemble': 1, 'because': 1, 'black': 1, 'build': 1, 'but': 1, 'cases': 1, 'central': 1, 'checking': 1, 'come': 1, 'concepts': 1, 'contains': 1, 'cope': 1, 'coronavirus': 2, 'could': 1, 'course': 2, 'customer': 1, 'data': 2, 'daughter': 1, 'death': 1, 'did': 1, 'diseases': 1, 'diy': 1, 'do': 1, 'don': 1, 'earth': 1, 'edge': 1, 'emotional': 5, 'empirically': 2, 'entire': 1, 'especially': 1, 'examples': 1, 'exist': 1, 'fault': 1, 'feature': 1, 'fever': 2, 'fine': 1, 'floor': 1, 'for': 2, 'forever': 1, 'from': 1, 'full': 1, 'gas': 1, 'girlhood': 1, 'golden': 1, 'gundam': 1, 'have': 1, 'heat': 1, 'heavens': 5, 'helping': 2, 'hobby': 1, 'how': 3, 'huge': 1, 'humans': 1, 'if': 2, 'in': 1, 'individual': 1, 'interested': 1, 'into': 1, 'ire': 1, 'is': 1, 'it': 1, 'journeys': 1, 'just': 1, 'leading': 1, 'like': 1, 'lines': 1, 'll': 2, 'many': 1, 'matter': 1, 'measure': 1, 'melting': 1, 'methane': 2, 'might': 2, 'model': 5, 'models': 2, 'mortem': 1, 'mother': 3, 'motherhood': 1, 'mothers': 1, 'near': 1, 'need': 4, 'negatively': 1, 'never': 1, 'new': 1, 'no': 1, 'not': 1, 'obviously': 1, 'of': 1, 'ones': 1, 'or': 1, 'origins': 1, 'other': 1, 'out': 1, 'outbreaks': 1, 'paint': 3, 'paradise': 1, 'passing': 1, 'past': 1, 'perfection': 1, 'perfectly': 6, 'permafrost': 1, 'piece': 1, 'place': 1, 'post': 2, 'potent': 2, 'purchase': 1, 'put': 3, 'react': 1, 'recommend': 1, 'releasing': 2, 'reveal': 3, 'right': 2, 'rising': 2, 'scarlet': 1, 'scientists': 1, 'share': 3, 'socialization': 1, 'spoke': 1, 'station': 1, 'statistical': 1, 'statistically': 1, 'such': 1, 'sums': 1, 'temperatures': 1, 'tend': 1, 'that': 1, 'thaws': 1} \n",
      "\n",
      "{'absence': 1, 'across': 1, 'actually': 1, 'age': 1, 'akiba': 1, 'akihabara': 1, 'almost': 1, 'also': 2, 'always': 1, 'an': 1, 'and': 4, 'are': 2, 'as': 2, 'ask': 1, 'asked': 1, 'assemble': 1, 'because': 1, 'black': 3, 'build': 1, 'but': 2, 'cases': 1, 'central': 1, 'checking': 1, 'come': 1, 'concepts': 1, 'contains': 1, 'cope': 1, 'coronavirus': 1, 'could': 1, 'course': 1, 'customer': 1, 'data': 1, 'daughter': 1, 'death': 1, 'did': 1, 'diseases': 1, 'diy': 1, 'do': 1, 'don': 1, 'earth': 1, 'edge': 1, 'emotional': 2, 'empirically': 1, 'entire': 1, 'especially': 1, 'examples': 1, 'exist': 1, 'fault': 1, 'feature': 1, 'fever': 1, 'fine': 1, 'floor': 1, 'for': 1, 'forever': 1, 'from': 1, 'full': 1, 'gas': 1, 'girlhood': 1, 'golden': 1, 'gundam': 1, 'have': 1, 'heat': 1, 'heavens': 1, 'helping': 1, 'hobby': 1, 'how': 2, 'huge': 1, 'humans': 1, 'if': 1, 'in': 3, 'individual': 1, 'interested': 1, 'into': 1, 'ire': 1, 'is': 4, 'it': 4, 'journeys': 1, 'just': 1, 'leading': 1, 'like': 1, 'lines': 1, 'll': 1, 'many': 3, 'matter': 1, 'measure': 1, 'melting': 1, 'methane': 1, 'might': 1, 'model': 3, 'models': 2, 'mortem': 1, 'mother': 1, 'motherhood': 1, 'mothers': 1, 'near': 1, 'need': 1, 'negatively': 1, 'never': 1, 'new': 1, 'no': 1, 'not': 2, 'obviously': 1, 'of': 10, 'ones': 2, 'or': 1, 'origins': 1, 'other': 1, 'out': 1, 'outbreaks': 1, 'paint': 1, 'paradise': 1, 'passing': 1, 'past': 1, 'perfection': 1, 'perfectly': 1, 'permafrost': 1, 'piece': 1, 'place': 1, 'post': 1, 'potent': 1, 'purchase': 1, 'put': 1, 'react': 1, 'recommend': 1, 'releasing': 1, 'reveal': 1, 'right': 1, 'rising': 1, 'scarlet': 1, 'scientists': 2, 'share': 1, 'socialization': 1, 'spoke': 1, 'station': 1, 'statistical': 1, 'statistically': 1, 'such': 1, 'sums': 1, 'temperatures': 1, 'tend': 1, 'that': 2, 'thaws': 1, 'the': 7, 'their': 2, 'them': 1, 'these': 1, 'think': 1, 'this': 3, 'time': 2, 'to': 8, 'trapping': 1, 'trouble': 1, 'uncommon': 1, 'understand': 1, 'vent': 1, 'want': 1, 'wellness': 1, 'were': 1, 'what': 1, 'while': 1, 'who': 1, 'will': 1, 'willing': 1, 'with': 3, 'writing': 1, 'yodobashi': 1, 'you': 5, 'your': 2, 'yourself': 1}\n"
     ]
    }
   ],
   "source": [
    "word_list_chat = c_vec.get_feature_names();    \n",
    "count_list_chat = X_chat.toarray().sum(axis=0) \n",
    "freq_chat = dict(zip(word_list_chat,count_list_chat))\n",
    "\n",
    "word_list_article = c_vec.get_feature_names();    \n",
    "count_list_article = X_article.toarray().sum(axis=0) \n",
    "freq_article = dict(zip(word_list_article,count_list_article))\n",
    "\n",
    "print(freq_chat,'\\n')\n",
    "print(freq_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [row['Sentence'] for index,row in data.iterrows()]\n",
    "\n",
    "X = c_vec.fit_transform(docs)\n",
    "\n",
    "total_features = len(c_vec.get_feature_names())\n",
    "total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 230\n"
     ]
    }
   ],
   "source": [
    "total_cnts_features_chat = count_list_chat.sum(axis=0)\n",
    "total_cnts_features_article = count_list_article.sum(axis=0)\n",
    "print(total_cnts_features_chat,total_cnts_features_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = 'I am coming to France. Anyone available this week?'\n",
    "new_word_list = word_tokenize(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 0.0021008403361344537,\n",
       " 'am': 0.0021008403361344537,\n",
       " 'coming': 0.0021008403361344537,\n",
       " 'to': 0.0021008403361344537,\n",
       " 'France': 0.0021008403361344537,\n",
       " '.': 0.0021008403361344537,\n",
       " 'Anyone': 0.0021008403361344537,\n",
       " 'available': 0.0021008403361344537,\n",
       " 'this': 0.0021008403361344537,\n",
       " 'week': 0.0021008403361344537,\n",
       " '?': 0.0021008403361344537}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_chat_with_ls = []\n",
    "for word in new_word_list:\n",
    "    if word in freq_chat.keys():\n",
    "        count = freq_chat[word]\n",
    "    else:\n",
    "        count = 0\n",
    "    prob_chat_with_ls.append((count + 1)/(total_cnts_features_chat + total_features))\n",
    "prob_chat_dict=dict(zip(new_word_list,prob_chat_with_ls))\n",
    "prob_chat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 0.001968503937007874,\n",
       " 'am': 0.001968503937007874,\n",
       " 'coming': 0.001968503937007874,\n",
       " 'to': 0.017716535433070866,\n",
       " 'France': 0.001968503937007874,\n",
       " '.': 0.001968503937007874,\n",
       " 'Anyone': 0.001968503937007874,\n",
       " 'available': 0.001968503937007874,\n",
       " 'this': 0.007874015748031496,\n",
       " 'week': 0.001968503937007874,\n",
       " '?': 0.001968503937007874}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_article_with_ls = []\n",
    "for word in new_word_list:\n",
    "    if word in freq_article.keys():\n",
    "        count = freq_article[word]\n",
    "    else:\n",
    "        count = 0\n",
    "    prob_article_with_ls.append((count + 1)/(total_cnts_features_article + total_features))\n",
    "prob_article_dict = dict(zip(new_word_list,prob_article_with_ls))\n",
    "prob_article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.81959639654562 64.9517769849072\n",
      "\"I am coming to France. Anyone available this week?\" is classified as Chat\n"
     ]
    }
   ],
   "source": [
    "print(log_prod_list(prob_chat_with_ls),log_prod_list(prob_article_with_ls))\n",
    "if(log_prod_list(prob_chat_with_ls)<log_prod_list(prob_article_with_ls)):\n",
    "    print(\"\\\"\"+str(new_sentence)+\" is classified as Article\")\n",
    "else:\n",
    "    print(\"\\\"\"+str(new_sentence)+\"\\\"\"\" is classified as Chat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
